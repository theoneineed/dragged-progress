{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-BoWTfidf.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNop62Tk+I2OheprOqM3F2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6ju_aDwMKzWf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKJOt4dmO9xI"},"source":["# Bag of Words (BoW)\n","\n","A bag-of-words model (BoW), is a way of extacting features from text for use in modeling, such as with machine learning algorithms. \n","\n","This is a very simple and flexible approach. It can be use in myriad of ways for extracting document. It involves two things:\n","\n","1. A vovabulary of known words.\n","2. A measure of the presence of known words. \n","\n","It is called a \"bag\" of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with wether known words occur in the document, not where in the document. \n","\n","Ex. \n","\n","* Betsy bought a butter\n","* but the butter was bitter\n","* so she added more butter to make bitter butter better.\n","\n","Unique Words:\n","\n","[Betsy, bought, a, butter, but, the, was, bitter, so, she, added, more, to, make, better]\n","\n","Ex: Betsy bought a butter: [1, 1, 1, 1, 0, 0, 0,0,0,0,0,0,0,0,0]\n","\n","Resulting vector is called the sparse matrix\n","\n","\n","* cleaning text\n","* ngrams\n","* Scoring words (ex: frequencies)\n","\n","Limitations of BoW:\n","\n","* If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n","* Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n","* We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text."]},{"cell_type":"markdown","metadata":{"id":"SmWm_OSMTrWK"},"source":["# TF-IDF (term frequency-inverse document frequency)\n","\n","TF-IDF is a stattistical measure the evaluates how relevant a word is to a document in a collection of documetns. \n","\n","This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n","\n","TF-IDF was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they donâ€™t mean much to that document in particular.\n","\n","### How is TF-IDF calculated?\n","\n","$$tf_{i,j} \\times log(\\frac{N}{df_{i}})$$\n","\n","* The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n","* The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n","* So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n","\n","## Application of TF-IDf\n","* Information retreival\n","* Keyword Extraction\n","\n","[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n","\n","![tfidf](https://drive.google.com/uc?id=17W0deYjn_BLGWY5lqBYKOrFFVm4uLkV)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sml-zNzIK0kH"},"source":["# Case Study: Sentiment Analysis"]},{"cell_type":"code","metadata":{"id":"Ig7qnbgcK3yS"},"source":["# from google.colab import drive\n","\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gdg-yuisK6HX"},"source":["# PATH = '/content/drive/MyDrive/NLPWorkShopANPAOct2021/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRgWT3tlLEG3"},"source":["\n","# import pandas as pd\n","# import numpy as np\n","\n","# # Read in the data\n","# df = pd.read_csv(PATH+'Amazon_Unlocked_Mobile.csv')\n","\n","# # Sample the data to speed up computation\n","# # Comment out this line to match with lecture\n","# df = df.sample(frac=0.1)\n","\n","# df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIpz1voJ7-3r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4wcP1ivLHlx"},"source":["# Drop missing values\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaYNKW4K8ENp"},"source":["# Remove any 'neutral' ratings equal to 3\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2xyE9CC8LVS"},"source":["\n","# Encode 4s and 5s as 1 (rated positively)\n","# Encode 1s and 2s as 0 (rated poorly)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-XJPwROLL5m"},"source":["# Most ratings are positive\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0wH2a87LQxc"},"source":["# from sklearn.model_selection import train_test_split\n","\n","# # Split data into training and test sets\n","# X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n","#                                                     df['Positively Rated'], \n","#                                                     random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIglq9UQ8lsw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oYrF8b7LSxC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nb7B01FxLYHw"},"source":["## CountVectorizer"]},{"cell_type":"code","metadata":{"id":"vhFytfgd9ZBM"},"source":["\n","# from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bo_Y4zRqLUsn"},"source":["\n","\n","# Fit the CountVectorizer to the training data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pp6IAGwzLbgx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBNDjqTCLdUa"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grTWX1w-Lg3t"},"source":["\n","# transform the documents in the training data to a document-term matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1ycTFws_XwE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOQZA4iaLi20"},"source":["\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.metrics import roc_auc_score\n","\n","# Train the model\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOujZU0zLlSs"},"source":["\n","\n","# Predict the transformed test documents\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsYFxAxHLrvS"},"source":["# get the feature names as numpy array\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmOpsAVF_2nh"},"source":["\n","# Sort the coefficients from the model\n","\n","# Find the 10 smallest and 10 largest coefficients\n","# The 10 largest coefficients are being indexed using [:-11:-1] \n","# so the list returned is in order of largest to smallest\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBMoZd1ULvcG"},"source":["\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# # Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n","# vect = TfidfVectorizer(min_df=5).fit(X_train)\n","# len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exkRI-O3L7HW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLu82Ux6L85m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2yZri_GL-pY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9QBBIo-MBU6"},"source":["# These reviews are treated the same by our current model\n","# print(model.predict(vect.transform(['not an issue, phone is working',\n","#                                     'an issue, phone is not working'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YTTM3q-MDa3"},"source":["# Fit the CountVectorizer to the training data specifiying a minimum \n","# document frequency of 5 and extracting 1-grams and 2-grams\n","# vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n","\n","# X_train_vectorized = vect.transform(X_train)\n","\n","# len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4iQmtOBMQcT"},"source":["# model = LogisticRegression()\n","# model.fit(X_train_vectorized, y_train)\n","\n","# predictions = model.predict(vect.transform(X_test))\n","\n","# print('AUC: ', roc_auc_score(y_test, predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFNqYXrCMSmR"},"source":["# feature_names = np.array(vect.get_feature_names())\n","\n","# sorted_coef_index = model.coef_[0].argsort()\n","\n","# print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n","# print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhYY4DHJMVjc"},"source":["# These reviews are now correctly identified\n","# print(model.predict(vect.transform(['not an issue, phone is working',\n","#                                     'an issue, phone is not working'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gljYWFi7MYK8"},"source":[""],"execution_count":null,"outputs":[]}]}