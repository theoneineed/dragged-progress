{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2-NLTK.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPcsfnVgKk5VumottUolzhs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dblGrE1R3gXW"},"source":["\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7eox9zN2LkU","executionInfo":{"status":"ok","timestamp":1633206475065,"user_tz":420,"elapsed":216,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}},"outputId":"1ae858ac-51a4-46a9-b310-12a8ccfe42a4"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"p4qZPy0Wa773","executionInfo":{"status":"ok","timestamp":1633206479165,"user_tz":420,"elapsed":268,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}}},"source":["PATH = '/content/drive/MyDrive/NLPWorkShopANPAOct2021/'"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7w_AOBmr3iJW"},"source":["### How to install NLTK on your local machine\n","\n","Both sets of instructions below assume you already have Python installed. These instructions are taken directly from [http://www.nltk.org/install.html](http://www.nltk.org/install.html).\n","\n","**Mac/Unix**\n","\n","From the terminal:\n","1. Install NLTK: run `pip3 install -U nltk`\n","2. Test installation: run `python` then type `import nltk`\n","\n","**Windows**\n","\n","1. Install NLTK: [http://pypi.python.org/pypi/nltk](http://pypi.python.org/pypi/nltk)\n","2. Test installation: `Start>Python35`, then type `import nltk`\n","\n","Both sets of instructions below assume you already have "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"REUPrfxP4e7w","executionInfo":{"status":"ok","timestamp":1633206628585,"user_tz":420,"elapsed":143108,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}},"outputId":"16d3b9b7-9adc-4224-b08d-853b759fa971"},"source":["import nltk\n","nltk.download()"],"execution_count":27,"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> all\n","    Downloading collection 'all'\n","       | \n","       | Downloading package abc to /root/nltk_data...\n","       |   Package abc is already up-to-date!\n","       | Downloading package alpino to /root/nltk_data...\n","       |   Package alpino is already up-to-date!\n","       | Downloading package biocreative_ppi to /root/nltk_data...\n","       |   Package biocreative_ppi is already up-to-date!\n","       | Downloading package brown to /root/nltk_data...\n","       |   Package brown is already up-to-date!\n","       | Downloading package brown_tei to /root/nltk_data...\n","       |   Package brown_tei is already up-to-date!\n","       | Downloading package cess_cat to /root/nltk_data...\n","       |   Package cess_cat is already up-to-date!\n","       | Downloading package cess_esp to /root/nltk_data...\n","       |   Package cess_esp is already up-to-date!\n","       | Downloading package chat80 to /root/nltk_data...\n","       |   Package chat80 is already up-to-date!\n","       | Downloading package city_database to /root/nltk_data...\n","       |   Package city_database is already up-to-date!\n","       | Downloading package cmudict to /root/nltk_data...\n","       |   Package cmudict is already up-to-date!\n","       | Downloading package comparative_sentences to\n","       |     /root/nltk_data...\n","       |   Package comparative_sentences is already up-to-date!\n","       | Downloading package comtrans to /root/nltk_data...\n","       |   Package comtrans is already up-to-date!\n","       | Downloading package conll2000 to /root/nltk_data...\n","       |   Package conll2000 is already up-to-date!\n","       | Downloading package conll2002 to /root/nltk_data...\n","       |   Package conll2002 is already up-to-date!\n","       | Downloading package conll2007 to /root/nltk_data...\n","       |   Package conll2007 is already up-to-date!\n","       | Downloading package crubadan to /root/nltk_data...\n","       |   Package crubadan is already up-to-date!\n","       | Downloading package dependency_treebank to /root/nltk_data...\n","       |   Package dependency_treebank is already up-to-date!\n","       | Downloading package dolch to /root/nltk_data...\n","       |   Package dolch is already up-to-date!\n","       | Downloading package europarl_raw to /root/nltk_data...\n","       |   Package europarl_raw is already up-to-date!\n","       | Downloading package floresta to /root/nltk_data...\n","       |   Package floresta is already up-to-date!\n","       | Downloading package framenet_v15 to /root/nltk_data...\n","       |   Package framenet_v15 is already up-to-date!\n","       | Downloading package framenet_v17 to /root/nltk_data...\n","       |   Package framenet_v17 is already up-to-date!\n","       | Downloading package gazetteers to /root/nltk_data...\n","       |   Package gazetteers is already up-to-date!\n","       | Downloading package genesis to /root/nltk_data...\n","       |   Package genesis is already up-to-date!\n","       | Downloading package gutenberg to /root/nltk_data...\n","       |   Package gutenberg is already up-to-date!\n","       | Downloading package ieer to /root/nltk_data...\n","       |   Package ieer is already up-to-date!\n","       | Downloading package inaugural to /root/nltk_data...\n","       |   Package inaugural is already up-to-date!\n","       | Downloading package indian to /root/nltk_data...\n","       |   Package indian is already up-to-date!\n","       | Downloading package jeita to /root/nltk_data...\n","       |   Package jeita is already up-to-date!\n","       | Downloading package kimmo to /root/nltk_data...\n","       |   Package kimmo is already up-to-date!\n","       | Downloading package knbc to /root/nltk_data...\n","       |   Package knbc is already up-to-date!\n","       | Downloading package lin_thesaurus to /root/nltk_data...\n","       |   Package lin_thesaurus is already up-to-date!\n","       | Downloading package mac_morpho to /root/nltk_data...\n","       |   Package mac_morpho is already up-to-date!\n","       | Downloading package machado to /root/nltk_data...\n","       |   Package machado is already up-to-date!\n","       | Downloading package masc_tagged to /root/nltk_data...\n","       |   Package masc_tagged is already up-to-date!\n","       | Downloading package moses_sample to /root/nltk_data...\n","       |   Package moses_sample is already up-to-date!\n","       | Downloading package movie_reviews to /root/nltk_data...\n","       |   Package movie_reviews is already up-to-date!\n","       | Downloading package names to /root/nltk_data...\n","       |   Package names is already up-to-date!\n","       | Downloading package nombank.1.0 to /root/nltk_data...\n","       |   Package nombank.1.0 is already up-to-date!\n","       | Downloading package nps_chat to /root/nltk_data...\n","       |   Package nps_chat is already up-to-date!\n","       | Downloading package omw to /root/nltk_data...\n","       |   Package omw is already up-to-date!\n","       | Downloading package opinion_lexicon to /root/nltk_data...\n","       |   Package opinion_lexicon is already up-to-date!\n","       | Downloading package paradigms to /root/nltk_data...\n","       |   Package paradigms is already up-to-date!\n","       | Downloading package pil to /root/nltk_data...\n","       |   Package pil is already up-to-date!\n","       | Downloading package pl196x to /root/nltk_data...\n","       |   Package pl196x is already up-to-date!\n","       | Downloading package ppattach to /root/nltk_data...\n","       |   Package ppattach is already up-to-date!\n","       | Downloading package problem_reports to /root/nltk_data...\n","       |   Package problem_reports is already up-to-date!\n","       | Downloading package propbank to /root/nltk_data...\n","       |   Package propbank is already up-to-date!\n","       | Downloading package ptb to /root/nltk_data...\n","       |   Package ptb is already up-to-date!\n","       | Downloading package product_reviews_1 to /root/nltk_data...\n","       |   Package product_reviews_1 is already up-to-date!\n","       | Downloading package product_reviews_2 to /root/nltk_data...\n","       |   Package product_reviews_2 is already up-to-date!\n","       | Downloading package pros_cons to /root/nltk_data...\n","       |   Package pros_cons is already up-to-date!\n","       | Downloading package qc to /root/nltk_data...\n","       |   Package qc is already up-to-date!\n","       | Downloading package reuters to /root/nltk_data...\n","       |   Package reuters is already up-to-date!\n","       | Downloading package rte to /root/nltk_data...\n","       |   Package rte is already up-to-date!\n","       | Downloading package semcor to /root/nltk_data...\n","       |   Package semcor is already up-to-date!\n","       | Downloading package senseval to /root/nltk_data...\n","       |   Package senseval is already up-to-date!\n","       | Downloading package sentiwordnet to /root/nltk_data...\n","       |   Package sentiwordnet is already up-to-date!\n","       | Downloading package sentence_polarity to /root/nltk_data...\n","       |   Package sentence_polarity is already up-to-date!\n","       | Downloading package shakespeare to /root/nltk_data...\n","       |   Package shakespeare is already up-to-date!\n","       | Downloading package sinica_treebank to /root/nltk_data...\n","       |   Package sinica_treebank is already up-to-date!\n","       | Downloading package smultron to /root/nltk_data...\n","       |   Package smultron is already up-to-date!\n","       | Downloading package state_union to /root/nltk_data...\n","       |   Package state_union is already up-to-date!\n","       | Downloading package stopwords to /root/nltk_data...\n","       |   Package stopwords is already up-to-date!\n","       | Downloading package subjectivity to /root/nltk_data...\n","       |   Package subjectivity is already up-to-date!\n","       | Downloading package swadesh to /root/nltk_data...\n","       |   Package swadesh is already up-to-date!\n","       | Downloading package switchboard to /root/nltk_data...\n","       |   Package switchboard is already up-to-date!\n","       | Downloading package timit to /root/nltk_data...\n","       |   Package timit is already up-to-date!\n","       | Downloading package toolbox to /root/nltk_data...\n","       |   Package toolbox is already up-to-date!\n","       | Downloading package treebank to /root/nltk_data...\n","       |   Package treebank is already up-to-date!\n","       | Downloading package twitter_samples to /root/nltk_data...\n","       |   Package twitter_samples is already up-to-date!\n","       | Downloading package udhr to /root/nltk_data...\n","       |   Package udhr is already up-to-date!\n","       | Downloading package udhr2 to /root/nltk_data...\n","       |   Package udhr2 is already up-to-date!\n","       | Downloading package unicode_samples to /root/nltk_data...\n","       |   Package unicode_samples is already up-to-date!\n","       | Downloading package universal_treebanks_v20 to\n","       |     /root/nltk_data...\n","       |   Package universal_treebanks_v20 is already up-to-date!\n","       | Downloading package verbnet to /root/nltk_data...\n","       |   Package verbnet is already up-to-date!\n","       | Downloading package verbnet3 to /root/nltk_data...\n","       |   Package verbnet3 is already up-to-date!\n","       | Downloading package webtext to /root/nltk_data...\n","       |   Package webtext is already up-to-date!\n","       | Downloading package wordnet to /root/nltk_data...\n","       |   Package wordnet is already up-to-date!\n","       | Downloading package wordnet_ic to /root/nltk_data...\n","       |   Package wordnet_ic is already up-to-date!\n","       | Downloading package words to /root/nltk_data...\n","       |   Package words is already up-to-date!\n","       | Downloading package ycoe to /root/nltk_data...\n","       |   Package ycoe is already up-to-date!\n","       | Downloading package rslp to /root/nltk_data...\n","       |   Package rslp is already up-to-date!\n","       | Downloading package maxent_treebank_pos_tagger to\n","       |     /root/nltk_data...\n","       |   Package maxent_treebank_pos_tagger is already up-to-date!\n","       | Downloading package universal_tagset to /root/nltk_data...\n","       |   Package universal_tagset is already up-to-date!\n","       | Downloading package maxent_ne_chunker to /root/nltk_data...\n","       |   Package maxent_ne_chunker is already up-to-date!\n","       | Downloading package punkt to /root/nltk_data...\n","       |   Package punkt is already up-to-date!\n","       | Downloading package book_grammars to /root/nltk_data...\n","       |   Package book_grammars is already up-to-date!\n","       | Downloading package sample_grammars to /root/nltk_data...\n","       |   Package sample_grammars is already up-to-date!\n","       | Downloading package spanish_grammars to /root/nltk_data...\n","       |   Package spanish_grammars is already up-to-date!\n","       | Downloading package basque_grammars to /root/nltk_data...\n","       |   Package basque_grammars is already up-to-date!\n","       | Downloading package large_grammars to /root/nltk_data...\n","       |   Package large_grammars is already up-to-date!\n","       | Downloading package tagsets to /root/nltk_data...\n","       |   Package tagsets is already up-to-date!\n","       | Downloading package snowball_data to /root/nltk_data...\n","       |   Package snowball_data is already up-to-date!\n","       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n","       |   Package bllip_wsj_no_aux is already up-to-date!\n","       | Downloading package word2vec_sample to /root/nltk_data...\n","       |   Package word2vec_sample is already up-to-date!\n","       | Downloading package panlex_swadesh to /root/nltk_data...\n","       |   Package panlex_swadesh is already up-to-date!\n","       | Downloading package mte_teip5 to /root/nltk_data...\n","       |   Package mte_teip5 is already up-to-date!\n","       | Downloading package averaged_perceptron_tagger to\n","       |     /root/nltk_data...\n","       |   Package averaged_perceptron_tagger is already up-to-date!\n","       | Downloading package averaged_perceptron_tagger_ru to\n","       |     /root/nltk_data...\n","       |   Package averaged_perceptron_tagger_ru is already up-to-\n","       |       date!\n","       | Downloading package perluniprops to /root/nltk_data...\n","       |   Package perluniprops is already up-to-date!\n","       | Downloading package nonbreaking_prefixes to\n","       |     /root/nltk_data...\n","       |   Package nonbreaking_prefixes is already up-to-date!\n","       | Downloading package vader_lexicon to /root/nltk_data...\n","       |   Package vader_lexicon is already up-to-date!\n","       | Downloading package porter_test to /root/nltk_data...\n","       |   Package porter_test is already up-to-date!\n","       | Downloading package wmt15_eval to /root/nltk_data...\n","       |   Package wmt15_eval is already up-to-date!\n","       | Downloading package mwa_ppdb to /root/nltk_data...\n","       |   Package mwa_ppdb is already up-to-date!\n","       | \n","     Done downloading collection all\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"GwRyGqqe4iub"},"source":["# dir(nltk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YHe9HBOkCNSD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxLsWbPmCNQh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbOYM5ao5iSx"},"source":["### What can you do with NLTK?"]},{"cell_type":"code","metadata":{"id":"fTM7tEQW5b1F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633206636620,"user_tz":420,"elapsed":142,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}},"outputId":"e82a6d45-bf10-4ee2-a24d-b44c0bb15285"},"source":["#stopwords\n","from nltk.corpus import stopwords\n","len(stopwords.words('english'))\n","# stopwords.words('english')"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"1xpQRboyrCYq","executionInfo":{"status":"ok","timestamp":1633206644854,"user_tz":420,"elapsed":156,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}}},"source":["# !ls\n","# % cd drive\n","# % cd MyDrive/\n","# !ls\n","# % cd NLPWorkShopANPAOct2021\n","# !ls"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mB-JNXmd54vd"},"source":["### Read in semi-structured text data"]},{"cell_type":"code","metadata":{"id":"rfS4HqND5rcR","colab":{"base_uri":"https://localhost:8080/","height":257},"executionInfo":{"status":"error","timestamp":1633206649140,"user_tz":420,"elapsed":361,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}},"outputId":"a677a2f5-4015-4404-ddfa-41e233772466"},"source":["# Read in the raw text\n","print(PATH)\n","rawData = open(PATH+ \"SMSSpamCollection.tsv\").read()"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLPWorkShopANPAOct2021/\n"]},{"output_type":"error","ename":"PermissionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-cc77d975a6ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in the raw text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrawData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"SMSSpamCollection.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted"]}]},{"cell_type":"code","metadata":{"id":"lyjocKx86E4s"},"source":["# Print the Raw data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S8RFDsAf6Jn0"},"source":["#split by \\t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4450TrgM6a5a"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEf5leGU6hgC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pF55k3zd605s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToX8MfXG67SZ","colab":{"base_uri":"https://localhost:8080/","height":618},"executionInfo":{"status":"error","timestamp":1633207016401,"user_tz":420,"elapsed":405,"user":{"displayName":"Shovit Bhari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8SHY80seEg1yKQZGE4rGmTfOLMow46uexTDDHgXs=s64","userId":"10536014893780956222"}},"outputId":"51bda399-df6f-4969-8134-514f851d1edd"},"source":["import pandas as pd\n","dataset = pd.read_csv(\"SMSSpamCollection.tsv\", sep=\"\\t\", header=None)\n","dataset.head()\n","\n"],"execution_count":32,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-1174c2e01126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SMSSpamCollection.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}]},{"cell_type":"code","metadata":{"id":"kqi2BIQV7OHV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"af8k4DJ87X5w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbPXWFCv7Z1o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSfcr6p_7dAA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJAcxodg7mqb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2mBOMNO7s-z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3YgFLra8adF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIM4xt7V8tx4"},"source":["# What is the shape of the dataset?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qv9x66Qm8zl-"},"source":["# How many spam/ham are there?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8TlFZZv85Zv"},"source":["# How much missing data is there?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9j0IU7188Ru"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPm6S64q8-M7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srInCAp99RvF"},"source":["# What does the cleaned version look like?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iE9peS3r9Ys2"},"source":[""],"execution_count":null,"outputs":[]}]}